{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ASSUMPTIONS:\n",
    "\n",
    "1. I am considering 'plot2-IoT Sensor Data' as my test set.\n",
    "2. 'plot2-Source 1 Weather' and 'plot2-Source 2 Weather' will be considered as train set.\n",
    "3. For sensor data, I will convert KPa to Pa for standard to remain same throughout.\n",
    "4. Datetime variable will be feature engineered.\n",
    "5. Target variable will be 'sensor' and will be made categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train1 = pd.read_csv('plot2-Source 1 Weather.csv')\n",
    "Train2 = pd.read_csv('plot2-Source 2 Weather.csv')\n",
    "Train = Train1.append(Train2, ignore_index=True) #Appended to form Main train set\n",
    "Train['datetime'] = Train['datetime'].astype('datetime64[ns]') #converting object type to datetime\n",
    "del Train1, Train2\n",
    "Train['sensor'] = Train['sensor'].astype('category') #converting dependent variable to categorical\n",
    "\n",
    "Test = pd.read_csv('plot2-IoT Sensor Data.csv')\n",
    "Test['value'] = Test['value']*1000 #all pressure are in Pa units\n",
    "Test['datetime'] = Test['datetime'].astype('datetime64[ns]') #converting object type to datetime\n",
    "Test['sensor'] = Test['sensor'].astype('category') #converting dependent variable to categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor</th>\n",
       "      <th>value</th>\n",
       "      <th>datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TC</td>\n",
       "      <td>16.5</td>\n",
       "      <td>2019-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HUM</td>\n",
       "      <td>44.0</td>\n",
       "      <td>2019-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PRES</td>\n",
       "      <td>1020.0</td>\n",
       "      <td>2019-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TC</td>\n",
       "      <td>17.3</td>\n",
       "      <td>2019-01-01 01:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HUM</td>\n",
       "      <td>43.0</td>\n",
       "      <td>2019-01-01 01:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sensor   value            datetime\n",
       "0     TC    16.5 2019-01-01 00:00:00\n",
       "1    HUM    44.0 2019-01-01 00:00:00\n",
       "2   PRES  1020.0 2019-01-01 00:00:00\n",
       "3     TC    17.3 2019-01-01 01:00:00\n",
       "4    HUM    43.0 2019-01-01 01:00:00"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor</th>\n",
       "      <th>value</th>\n",
       "      <th>datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HUM</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>2019-01-01 00:26:26.749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HUM</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>2019-01-01 01:26:26.650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HUM</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>2019-01-01 02:26:27.049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HUM</td>\n",
       "      <td>89860.0</td>\n",
       "      <td>2019-01-01 03:26:26.966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HUM</td>\n",
       "      <td>36460.0</td>\n",
       "      <td>2019-01-01 04:26:27.397</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sensor     value                datetime\n",
       "0    HUM  100000.0 2019-01-01 00:26:26.749\n",
       "1    HUM  100000.0 2019-01-01 01:26:26.650\n",
       "2    HUM  100000.0 2019-01-01 02:26:27.049\n",
       "3    HUM   89860.0 2019-01-01 03:26:26.966\n",
       "4    HUM   36460.0 2019-01-01 04:26:27.397"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering done on datetime variable:\n",
    "\n",
    "1. Not creating year variable as test data had only 2019 as year, redundant in this case.\n",
    "2. Not creating seconds variable as it is highly volatile in distribution and can cause trouble.\n",
    "3. After creating new variables, will drop datetime variable.\n",
    "4. Will one hot encode weekday variable.\n",
    "5. Normalizing our independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int32 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int32 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "Train['month'] = Train['datetime'].dt.month\n",
    "Train['day'] = Train['datetime'].dt.day\n",
    "Train['hour'] = Train['datetime'].dt.hour\n",
    "Train['minute'] = Train['datetime'].dt.minute\n",
    "Train['weekday'] = Train['datetime'].dt.day_name()\n",
    "onehotTrain = pd.get_dummies(Train['weekday'])\n",
    "Train = Train.drop('weekday',axis = 1)\n",
    "Train = Train.join(onehotTrain)\n",
    "del Train['datetime']\n",
    "Train.iloc[:,1:] = Train.iloc[:,1:].astype(int)\n",
    "Train.iloc[:,1:] = min_max_scaler.fit_transform(Train.iloc[:,1:].values)\n",
    "\n",
    "Test['month'] = Test['datetime'].dt.month\n",
    "Test['day'] = Test['datetime'].dt.day\n",
    "Test['hour'] = Test['datetime'].dt.hour\n",
    "Test['minute'] = Test['datetime'].dt.minute\n",
    "Test['weekday'] = Test['datetime'].dt.day_name()\n",
    "onehotTest = pd.get_dummies(Test['weekday'])\n",
    "Test = Test.drop('weekday',axis = 1)\n",
    "Test = Test.join(onehotTest)\n",
    "del Test['datetime']\n",
    "Test.iloc[:,1:] = Test.iloc[:,1:].astype(int)\n",
    "Test.iloc[:,1:] = min_max_scaler.fit_transform(Test.iloc[:,1:].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52419, 13)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24966, 13)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting original Train data into Train/Test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Train.iloc[:,1:]\n",
    "y = Train.iloc[:,0]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method one:\n",
    "    \n",
    "Running a base model. We can choose any model however here I am selecting RandomForestClassifier for classification\n",
    "and then calculate it's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to fit is:  0.3071005344390869  seconds\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "\n",
    "start_time = time.time()\n",
    "clf.fit(X_train, y_train)\n",
    "print('Time taken to fit is: ', time.time() - start_time, ' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is:  86.2552460892789  percent\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "print('Accuracy is: ', accuracy_score(y_test, y_pred)*100, ' percent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method Two:\n",
    "\n",
    "Selecting best base model and improving it further using RandomizedSearchCV and GridSearchCV.\n",
    "I am considering SVC and RandomForestClassifier as example, we can add more classifiers too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68.15707349777222\n",
      "##################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6257307529449463\n",
      "##################\n",
      "Best Model is:  RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "list_of_models = [SVC(), RandomForestClassifier()]\n",
    "list_of_acc = []\n",
    "\n",
    "for i in list_of_models:\n",
    "    start_time = time.time()\n",
    "    i.fit(X_train, y_train)\n",
    "    print(time.time() - start_time)\n",
    "\n",
    "    predicted = i.predict(X_test) ## Prediction of data\n",
    "        \n",
    "    cc = accuracy_score(y_test, predicted)\n",
    "    list_of_acc.append(cc)\n",
    "    print('##################')\n",
    "\n",
    "a = max([(v,i) for i,v in enumerate(list_of_acc)])\n",
    "b = a[1]\n",
    "c = list_of_models[b]\n",
    "print('Best Model is: ', c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating parameters grid for above base models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter grid for above selected best base model:  {'n_estimators': [10, 100, 1000], 'criterion': ['gini', 'entropy'], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 5], 'max_leaf_nodes': [100, 1000], 'bootstrap': [True, False]}\n"
     ]
    }
   ],
   "source": [
    "gridSVC = {'kernel': ['linear', 'rbf', 'sigmoid'],\n",
    "               'degree': [2,3,4,5],\n",
    "               'shrinking': [True,False],\n",
    "               'probability' : [True,False],\n",
    "               'decision_function_shape': ['ovr', 'ovo'],\n",
    "               'random_state' : [44,47,48,50,51]}\n",
    "\n",
    "#----------------------------------------------------------------------------#\n",
    "\n",
    "gridRandomForestClassifier = {'n_estimators': [10,100,1000],\n",
    "               'criterion': ['gini', 'entropy'],\n",
    "               'min_samples_split': [2,5,10],\n",
    "               'min_samples_leaf': [1,2,5],\n",
    "               'max_leaf_nodes' : [100,1000],\n",
    "               'bootstrap' : [True,False]}\n",
    "\n",
    "list_of_param = [gridSVC,\n",
    "                 gridRandomForestClassifier]\n",
    "\n",
    "d = a[1]\n",
    "e = list_of_param[d]\n",
    "print('Parameter grid for above selected best base model: ', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try RandomizedSearchCV with cross validation as 3 over 5 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to fit is:  9.931749105453491  seconds\n",
      "Accuracy with RandomizedSearchCV:  95.41205646699733 percent\n"
     ]
    }
   ],
   "source": [
    "op = RandomizedSearchCV(c, e, cv=3, random_state=42, n_iter=5)\n",
    "\n",
    "start_time = time.time()\n",
    "op.fit(X_train, y_train) \n",
    "print('Time taken to fit is: ', time.time() - start_time, ' seconds')\n",
    "\n",
    "predicted = op.predict(X_test)\n",
    "print('Accuracy with RandomizedSearchCV: ', accuracy_score(y_test, predicted)*100, 'percent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see we already got really better accuracy compared to just base model.\n",
    "Let us try GridSearchCV on same base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to fit is:  9472.738417387009  seconds\n",
      "Accuracy with GridSearchCV:  96.19420068676078 percent\n"
     ]
    }
   ],
   "source": [
    "op =  GridSearchCV(c, e, cv=3)\n",
    "\n",
    "start_time = time.time()\n",
    "op.fit(X_train, y_train) \n",
    "print('Time taken to fit is: ', time.time() - start_time, ' seconds')\n",
    "\n",
    "predicted = op.predict(X_test)\n",
    "print('Accuracy with GridSearchCV: ', accuracy_score(y_test, predicted)*100, 'percent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also try HyperOpt if needed based on our data, what is our objective etc instead of RandomizedSearchCV or GridSearchCV."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
